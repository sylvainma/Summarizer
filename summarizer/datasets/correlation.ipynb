{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation exepriments from the paper [Rethinking the Evaluation of Video Summaries](https://arxiv.org/abs/1903.11328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "sys.path.append(\"../../\")\n",
    "from summarizer.vsum_tools import evaluate_scores, upsample\n",
    "from summarizer.utils.io import load_tvsum_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".h5:  AwmHb44_ouw (20, 10597) 10597\n.mat:  AwmHb44_ouw (20, 10597) 10597\n"
    }
   ],
   "source": [
    "dataset = h5py.File(\"summarizer_dataset_tvsum_google_pool5.h5\", \"r\")\n",
    "video_1_h5 = dataset[\"video_1\"]\n",
    "video_name_h5 = video_1_h5[\"video_name\"][...].item()\n",
    "n_frames_h5 = video_1_h5[\"n_frames\"][...].item()\n",
    "positions_h5 = video_1_h5[\"picks\"][...]\n",
    "gtscore_h5 = np.expand_dims(upsample(video_1_h5[\"gtscore\"][...], n_frames_h5, positions_h5), axis=0)\n",
    "\n",
    "dataset = load_tvsum_mat(\"videos/tvsum/ydata-tvsum50.mat\")\n",
    "video_1_mat = dataset[0]\n",
    "video_name_mat = video_1_mat[\"video\"]\n",
    "n_frames_mat = int(video_1_mat[\"nframes\"])\n",
    "\n",
    "user_summary = video_1_h5[\"user_summary\"][...] # (n_users, n_frames) [0 1 ... 1]\n",
    "user_scores = video_1_mat[\"user_anno\"].T       # (n_users, n_frames) [1 5 ... 2]\n",
    "user_scores = (user_scores-1.0)/(5.0-1.0)      # normalize them \n",
    "n_users, n_frames = user_scores.shape\n",
    "\n",
    "assert video_name_h5 == video_name_mat\n",
    "assert n_frames_h5 == n_frames_mat\n",
    "\n",
    "print(\".h5: \", video_name_h5, user_summary.shape, n_frames)\n",
    "print(\".mat: \", video_name_mat, user_scores.shape, n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using summaries `[0 1 ... 1]` as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.0008349983749936529"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Random scores: should be 0\n",
    "corrs = []\n",
    "for _ in range(50):\n",
    "    machine_scores = np.random.uniform(0, 1, (n_frames,))\n",
    "    corrs.append(evaluate_scores(machine_scores, user_summary))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.17628410427445923"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Human scores: should be maximum\n",
    "# Leave-one-out strategy\n",
    "corrs = []\n",
    "for i in range(n_users):\n",
    "    machine_scores = user_scores[i]\n",
    "    corrs.append(evaluate_scores(machine_scores, np.delete(user_summary, i, 0)))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scores `[0.8 0.3 ... 0.1]` as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0003529005001347934"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Random scores: should be 0\n",
    "corrs = []\n",
    "for _ in range(50):\n",
    "    machine_scores = np.random.uniform(0, 1, (n_frames,))\n",
    "    corrs.append(evaluate_scores(machine_scores, user_scores))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2740630927883905"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Human scores: should be maximum\n",
    "# Leave-one-out strategy\n",
    "corrs = []\n",
    "for i in range(n_users):\n",
    "    machine_scores = user_scores[i]\n",
    "    corrs.append(evaluate_scores(machine_scores, np.delete(user_scores, i, 0)))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Observations:_\n",
    "* Using `user_summary` is consistent: random is near 0 and human is high (~0.17)\n",
    "* Using `user_scores` matches the paper: random also near 0 and human close to 0.26 from paper (us ~0.27)\n",
    "\n",
    "In the paper they have 0.26 so they might have averaged over all videos in TVSum.\n",
    "\n",
    "We should use `user_scores` for correlation because `user_summary` is actually a summary, so KTS+Knapsack+15% constraint have been used. They do not reflect the absolute importance of a single frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SumMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".h5:  b'Air_Force_One' (15, 4494) (1, 4494) 4494\n"
    }
   ],
   "source": [
    "dataset = h5py.File(\"summarizer_dataset_summe_google_pool5.h5\", \"r\")\n",
    "video_1 = dataset[\"video_1\"]\n",
    "video_name = video_1[\"video_name\"][...].item()\n",
    "n_frames = video_1[\"n_frames\"][...]\n",
    "positions = video_1[\"picks\"][...]\n",
    "\n",
    "user_summary = video_1[\"user_summary\"][...] # (n_users, n_frames)\n",
    "# For SumMe, we consider that all users annotated the same scores, the gtscore\n",
    "# Transform it to (1, n_frames)\n",
    "user_scores = np.expand_dims(upsample(video_1[\"gtscore\"][...], n_frames, positions), axis=0)\n",
    "\n",
    "print(\".h5: \", video_name, user_summary.shape, user_scores.shape, n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using summaries `[0 1 ... 1]` as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0008523037834626095"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Random scores: should be 0\n",
    "corrs = []\n",
    "for _ in range(50):\n",
    "    machine_scores = np.random.uniform(0, 1, (n_frames,))\n",
    "    corrs.append(evaluate_scores(machine_scores, user_summary))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.22560639713572397"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Human scores: should be maximum\n",
    "corrs = []\n",
    "for i in range(n_users):\n",
    "    machine_scores = user_summary[i] # here we don't have scores from user but only summary\n",
    "    corrs.append(evaluate_scores(machine_scores, np.delete(user_summary, i, 0)))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scores `[0.8 0.3 ... 0.1]` as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0009742716877428895"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Random scores: should be 0\n",
    "corrs = []\n",
    "for _ in range(50):\n",
    "    machine_scores = np.random.uniform(0, 1, (n_frames,))\n",
    "    corrs.append(evaluate_scores(machine_scores, user_scores))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3319613038488442"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Human scores: should be maximum\n",
    "corrs = []\n",
    "for i in range(n_users):\n",
    "    machine_scores = user_summary[i] # here we don't have scores from user but only summary\n",
    "    user_scores_ = np.expand_dims(np.delete(user_summary, i, 0).mean(axis=0), axis=0)\n",
    "    corrs.append(evaluate_scores(machine_scores, user_scores_))\n",
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Observations:_\n",
    "* Using `user_summary` is consistent: random scores leads to 0 and human is high (~0.22)\n",
    "* Using `user_scores` matches the paper: random also near 0 and human high (us ~0.33)\n",
    "\n",
    "In the paper they did not experiment this with SumMe since there is no importance score annotations.\n",
    "\n",
    "Here, we should use `user_scores` (in other words `/gtscore`) to be consistent with TVSum, even if `/gtscore` is computed from summaries of annotators instead of absolute frame importance score. But, these summaries were made by annotators directly, without usiing KTS. So it is not biased by KTS. Which is the dark side of using F-score. So we can assume were're fine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About correlation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([0.9, 0.3, 0.7]), array([0.4, 0.8, 1. ]))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x = np.asarray([0.9,0.3,0.7])\n",
    "y = np.asarray([0.4,0.8,1.0])\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "KendalltauResult(correlation=-0.33333333333333337, pvalue=1.0)\nKendalltauResult(correlation=-0.33333333333333337, pvalue=1.0)\nKendalltauResult(correlation=-0.33333333333333337, pvalue=1.0)\n"
    }
   ],
   "source": [
    "print(stats.kendalltau(stats.rankdata(-1*x), stats.rankdata(-1*y)))\n",
    "print(stats.kendalltau(stats.rankdata(x), stats.rankdata(y)))\n",
    "print(stats.kendalltau(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SpearmanrResult(correlation=-0.5, pvalue=0.6666666666666667)\nSpearmanrResult(correlation=-0.5, pvalue=0.6666666666666667)\nSpearmanrResult(correlation=-0.5, pvalue=0.6666666666666667)\n"
    }
   ],
   "source": [
    "print(stats.spearmanr(stats.rankdata(-1*x), stats.rankdata(-1*y)))\n",
    "print(stats.spearmanr(stats.rankdata(x), stats.rankdata(y)))\n",
    "print(stats.spearmanr(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: ranking and/or sorting seems to have no effect"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitvs37conda74d6c56bdbe144ada303560869af001c",
   "display_name": "Python 3.7.6 64-bit ('VS37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}